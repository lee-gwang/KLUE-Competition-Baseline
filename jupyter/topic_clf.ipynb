{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0343a98b",
   "metadata": {},
   "source": [
    "## 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9892bce4",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ------ LIBRARY -------#\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "import cv2\n",
    "# torch\n",
    "import torch\n",
    "import torch.cuda.amp as amp\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import *\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau, MultiStepLR, OneCycleLR\n",
    "#\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "import torch_optimizer as optim\n",
    "from collections import defaultdict\n",
    "import itertools as it\n",
    "\n",
    "import tqdm\n",
    "import random\n",
    "#import time\n",
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "# transformer\n",
    "from transformers import XLMPreTrainedModel, XLMRobertaModel, XLMRobertaConfig, XLMRobertaTokenizer\n",
    "from transformers import XLMRobertaForSequenceClassification, BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertForSequenceClassification, DistilBertForSequenceClassification, XLNetForSequenceClassification,\\\n",
    "XLMRobertaForSequenceClassification, XLMForSequenceClassification, RobertaForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f87869f9",
   "metadata": {
    "code_folding": [
     33
    ]
   },
   "outputs": [],
   "source": [
    "# class args\n",
    "class args:\n",
    "    # ---- factor ---- #\n",
    "    debug=False\n",
    "    amp = True\n",
    "    gpu = '1'\n",
    "    \n",
    "    epochs=10\n",
    "    batch_size=64\n",
    "    weight_decay=1e-6\n",
    "    n_fold=5\n",
    "    fold=3 # [0, 1, 2, 3, 4] # 원래는 3\n",
    "    \n",
    "    exp_name = 'experiment_name_folder'\n",
    "    dir_ = f'./saved_models/'\n",
    "    pt = 'klue/roberta-large' # ['klue/roberta-base','klue/roberta-small','klue/roberta-large', 'klue/roberta-large']\n",
    "    \n",
    "    max_len = 33\n",
    "    \n",
    "    start_lr = 1e-5#1e-3,5e-5\n",
    "    min_lr=1e-6\n",
    "    # ---- Dataset ---- #\n",
    "\n",
    "    # ---- Else ---- #\n",
    "    num_workers=8\n",
    "    seed=2021\n",
    "    scheduler = None#'get_linear_schedule_with_warmup'\n",
    "\n",
    "\n",
    "data_dir = './'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##----------------\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False # for faster training, but not deterministic\n",
    "\n",
    "set_seeds(seed=args.seed)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "299e9265",
   "metadata": {
    "code_folding": [
     1,
     13
    ]
   },
   "outputs": [],
   "source": [
    "# - util - #\n",
    "def get_learning_rate(optimizer):\n",
    "    lr=[]\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr +=[ param_group['lr'] ]\n",
    "\n",
    "    assert(len(lr)==1) #we support only one param_group\n",
    "    lr = lr[0]\n",
    "\n",
    "    return lr\n",
    "\n",
    "# data processing\n",
    "\n",
    "def load_data():\n",
    "    train_df = []\n",
    "    test_df = []\n",
    "\n",
    "    # train\n",
    "    #\n",
    "    df = pd.DataFrame()\n",
    "    df['contents'] = pd.read_json('./data/1.Training/라벨링데이터/TL1/안전건설/안전건설_93747.json')['documents'].apply(lambda x: x['Q_refined'])\n",
    "    df['label'] = 0\n",
    "    train_df.append(df)\n",
    "\n",
    "    #\n",
    "    df = pd.DataFrame()\n",
    "    df['contents'] = pd.read_json('./data/1.Training/라벨링데이터/TL1/교통/교통_85465.json')['documents'].apply(lambda x: x['Q_refined'])\n",
    "    df['label'] = 1\n",
    "    train_df.append(df)\n",
    "\n",
    "    #\n",
    "    df = pd.DataFrame()\n",
    "    df['contents'] = pd.read_json('./data/1.Training/라벨링데이터/TL1/건축허가/건축허가_57256.json')['documents'].apply(lambda x: x['Q_refined'])\n",
    "    df['label'] = 2\n",
    "    train_df.append(df)\n",
    "\n",
    "    #\n",
    "    df = pd.DataFrame()\n",
    "    df['contents'] = pd.read_json('./data/1.Training/라벨링데이터/TL1/환경미화/환경미화_38129.json')['documents'].apply(lambda x: x['Q_refined'])\n",
    "    df['label'] = 3\n",
    "    train_df.append(df)\n",
    "\n",
    "    # val\n",
    "    #\n",
    "    df = pd.DataFrame()\n",
    "    df['contents'] = pd.read_json('./data/2.Validation/라벨링데이터/VL1/안전건설/안전건설_11719.json')['documents'].apply(lambda x: x['Q_refined'])\n",
    "    df['label'] = 0\n",
    "    test_df.append(df)\n",
    "\n",
    "    #\n",
    "    df = pd.DataFrame()\n",
    "    df['contents'] = pd.read_json('./data/2.Validation/라벨링데이터/VL1/교통/교통_10683.json')['documents'].apply(lambda x: x['Q_refined'])\n",
    "    df['label'] = 1\n",
    "    test_df.append(df)\n",
    "\n",
    "    #\n",
    "    df = pd.DataFrame()\n",
    "    df['contents'] = pd.read_json('./data/2.Validation/라벨링데이터/VL1/건축허가/건축허가_7157.json')['documents'].apply(lambda x: x['Q_refined'])\n",
    "    df['label'] = 2\n",
    "    test_df.append(df)\n",
    "\n",
    "    #\n",
    "    df = pd.DataFrame()\n",
    "    df['contents'] = pd.read_json('./data/2.Validation/라벨링데이터/VL1/환경미화/환경미화_4766.json')['documents'].apply(lambda x: x['Q_refined'])\n",
    "    df['label'] = 3\n",
    "    test_df.append(df)\n",
    "    \n",
    "    \n",
    "    train_df = pd.concat(train_df).reset_index(drop=True)\n",
    "    test_df = pd.concat(test_df).reset_index(drop=True)\n",
    "    \n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "    train_df['fold'] = -1\n",
    "    for n_fold, (_,v_idx) in enumerate(skf.split(train_df, train_df['label'])):\n",
    "        train_df.loc[v_idx, 'fold']  = n_fold\n",
    "    train_df['id'] = [x for x in range(len(train_df))]\n",
    "    \n",
    "    \n",
    "    return train_df, test_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9010858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86ed46c9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac556bf5",
   "metadata": {
    "code_folding": [
     0,
     17
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def bert_tokenizer(sent, MAX_LEN, tokenizer):\n",
    "    \n",
    "    encoded_dict=tokenizer.encode_plus(\n",
    "    text = sent, \n",
    "    add_special_tokens=True, \n",
    "    max_length=MAX_LEN, \n",
    "    pad_to_max_length=True, \n",
    "    return_attention_mask=True,\n",
    "    truncation = True)\n",
    "    \n",
    "    input_id=encoded_dict['input_ids']\n",
    "    attention_mask=encoded_dict['attention_mask']\n",
    "    #token_type_id = encoded_dict['token_type_ids']\n",
    "    token_type_id = 0\n",
    "    \n",
    "    return input_id, attention_mask, token_type_id\n",
    "\n",
    "def preprocessing(train, type='train'):\n",
    "    \n",
    "    pt = args.pt#'monologg/kobert'\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.pt)\n",
    "    \n",
    "    MAX_LEN = args.max_len\n",
    "#     train = pd.read_csv('./train_data.csv')\n",
    "#     train=train[['title','topic_idx']]\n",
    "\n",
    "    input_ids =[]\n",
    "    attention_masks =[]\n",
    "    token_type_ids =[]\n",
    "    train_data_labels = []\n",
    "\n",
    "    for train_sent, train_label in tqdm.tqdm(zip(train['contents'], train['label'])):\n",
    "        try:\n",
    "            input_id, attention_mask,_ = bert_tokenizer(train_sent, MAX_LEN=MAX_LEN, tokenizer=tokenizer)\n",
    "\n",
    "            input_ids.append(input_id)\n",
    "            attention_masks.append(attention_mask)\n",
    "            token_type_ids.append(0)\n",
    "            #########################################\n",
    "            train_data_labels.append(train_label)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "    train_input_ids=np.array(input_ids, dtype=int)\n",
    "    train_attention_masks=np.array(attention_masks, dtype=int)\n",
    "    train_token_type_ids=np.array(token_type_ids, dtype=int)\n",
    "    ###########################################################\n",
    "    train_inputs=(train_input_ids, train_attention_masks, train_token_type_ids)\n",
    "    train_labels=np.asarray(train_data_labels, dtype=np.int32)\n",
    "\n",
    "    # save\n",
    "    train_data = {}\n",
    "\n",
    "    train_data['input_ids'] = train_input_ids\n",
    "    train_data['attention_mask'] = train_attention_masks\n",
    "    train_data['token_type_ids'] = train_token_type_ids\n",
    "    train_data['targets'] = np.asarray(train_data_labels, dtype=np.int32)\n",
    "    \n",
    "    os.makedirs(f'./data/{pt}/', exist_ok=True)\n",
    "    with open(f'./data/{pt}/{type}_data_{MAX_LEN}.pickle', 'wb') as f:\n",
    "        pickle.dump(train_data, f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caa8a5f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5f3800",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd7db76a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea55f1ec4ec4e87841f26911cd715cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/375 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13de047a1bf47ae9d5e668938de6293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/243k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c9b150a126410e9269306eb730dada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/734k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1643d8c7898744fb8ec4d896a3053507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/leegwang/anaconda3/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2285: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "274597it [00:29, 9403.33it/s] \n",
      "34325it [00:03, 9864.71it/s] \n"
     ]
    }
   ],
   "source": [
    "# ['monologg/kobert','klue/roberta-base','klue/roberta-small','klue/roberta-large','xlm-roberta-large', \n",
    "#            'bert-base-multilingual-uncased', 'klue/roberta-large']\n",
    "\n",
    "\n",
    "# train_df, test_df = load_data()\n",
    "# preprocessing(train_df)\n",
    "# preprocessing(test_df, type='test')\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbff676f",
   "metadata": {},
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de6900c5",
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "#  dataset\n",
    "# ------------------------\n",
    "class KobertDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, data, test=False):\n",
    "        \n",
    "        self.data = data\n",
    "        self.test = test\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.data['input_ids'].shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        ids = torch.tensor(self.data['input_ids'][idx], dtype=torch.long)\n",
    "        mask = torch.tensor(self.data['attention_mask'][idx], dtype=torch.long)\n",
    "        token_type_ids = torch.tensor(self.data['token_type_ids'][idx], dtype=torch.long)\n",
    "         \n",
    "            \n",
    "        if self.test:\n",
    "            return {\n",
    "                'ids': ids,\n",
    "                'mask': mask,\n",
    "                'token_type_ids': token_type_ids\n",
    "            }\n",
    "        \n",
    "        else:\n",
    "            target = torch.tensor(self.data['targets'][idx],dtype=torch.long)\n",
    "\n",
    "            return {\n",
    "                    'ids': ids,\n",
    "                    'mask': mask,\n",
    "                    'token_type_ids': token_type_ids,\n",
    "                    'targets': target\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd0f29",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89dee00d",
   "metadata": {
    "code_folding": [
     4,
     46,
     71
    ]
   },
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "#  scheduler\n",
    "# ------------------------\n",
    "\n",
    "def do_valid(net, valid_loader):\n",
    "\n",
    "    val_loss = 0\n",
    "    target_lst = []\n",
    "    pred_lst = []\n",
    "    logit = []\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    net.eval()\n",
    "    start_timer = timer()\n",
    "    for t, data in enumerate(tqdm.tqdm(valid_loader)):\n",
    "        ids  = data['ids'].to(device)\n",
    "        mask  = data['mask'].to(device)\n",
    "        tokentype = data['token_type_ids'].to(device)\n",
    "        target = data['targets'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if args.amp:\n",
    "                with amp.autocast():\n",
    "                    # output\n",
    "                    output = net(ids, mask)\n",
    "                    output = output[0]\n",
    "\n",
    "                    # loss\n",
    "                    loss = loss_fn(output, target)\n",
    "\n",
    "            else:\n",
    "                output = net(ids, mask)#.squeeze(0)\n",
    "                loss = loss_fn(output, target)\n",
    "            \n",
    "            val_loss += loss\n",
    "            target_lst.extend(target.detach().cpu().numpy())\n",
    "            pred_lst.extend(output.argmax(dim=1).tolist())\n",
    "            logit.extend(output.tolist())\n",
    "            \n",
    "        val_mean_loss = val_loss / len(valid_loader)\n",
    "        validation_score = f1_score(y_true=target_lst, y_pred=pred_lst, average='macro')\n",
    "        validation_acc = accuracy_score(y_true=target_lst, y_pred=pred_lst)\n",
    "        \n",
    "\n",
    "    return val_mean_loss, validation_score, validation_acc, logit\n",
    "\n",
    "def do_predict(net, valid_loader):\n",
    "    \n",
    "    val_loss = 0\n",
    "    pred_lst = []\n",
    "    logit=[]\n",
    "    net.eval()\n",
    "    for t, data in enumerate(tqdm.tqdm(valid_loader)):\n",
    "        ids  = data['ids'].to(device)\n",
    "        mask  = data['mask'].to(device)\n",
    "        tokentype = data['token_type_ids'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if args.amp:\n",
    "                with amp.autocast():\n",
    "                    # output\n",
    "                    output = net(ids, mask)[0]\n",
    "\n",
    "            else:\n",
    "                output = net(ids, mask)\n",
    "             \n",
    "            pred_lst.extend(output.argmax(dim=1).tolist())\n",
    "            logit.extend(output.tolist())\n",
    "            \n",
    "    return pred_lst,logit\n",
    "\n",
    "def run_train(folds=3):\n",
    "    out_dir = args.dir_+ f'/fold{args.fold}/{args.exp_name}/'\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    # load dataset\n",
    "    train, test = load_data()    \n",
    "    with open(f'./data/{args.pt}/train_data_{args.max_len}.pickle', 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "    with open(f'./data/{args.pt}/test_data_{args.max_len}.pickle', 'rb') as f:\n",
    "        test_data = pickle.load(f)    \n",
    "    \n",
    "    # split fold\n",
    "    for n_fold in range(5):\n",
    "        if n_fold != folds:\n",
    "            print(f'{n_fold} fold pass'+'\\n')\n",
    "            continue\n",
    "            \n",
    "        if args.debug:\n",
    "            train = train.sample(1000).copy()\n",
    "        \n",
    "        trn_idx = train[train['fold']!=n_fold]['id'].values\n",
    "        val_idx = train[train['fold']==n_fold]['id'].values\n",
    "    \n",
    "\n",
    "        train_dict = {'input_ids' : train_data['input_ids'][trn_idx] , 'attention_mask' : train_data['attention_mask'][trn_idx] , \n",
    "                      'token_type_ids' : train_data['token_type_ids'][trn_idx], 'targets' : train_data['targets'][trn_idx]}\n",
    "#         val_dict = {'input_ids' : train_data['input_ids'][val_idx] , 'attention_mask' : train_data['attention_mask'][val_idx] , \n",
    "#                       'token_type_ids' : train_data['token_type_ids'][val_idx], 'targets' : train_data['targets'][val_idx]}\n",
    "        val_dict = {'input_ids' : test_data['input_ids'] , 'attention_mask' : test_data['attention_mask'] , \n",
    "                      'token_type_ids' : test_data['token_type_ids'], 'targets' : test_data['targets']}\n",
    "        ## dataset ------------------------------------\n",
    "        train_dataset = KobertDataSet(data = train_dict)\n",
    "        valid_dataset = KobertDataSet(data = val_dict)\n",
    "        trainloader = DataLoader(dataset=train_dataset, batch_size=args.batch_size,\n",
    "                                 num_workers=8, shuffle=True, pin_memory=True)\n",
    "        validloader = DataLoader(dataset=valid_dataset, batch_size=args.batch_size, \n",
    "                                 num_workers=8, shuffle=False, pin_memory=True)\n",
    "\n",
    "        ## net ----------------------------------------\n",
    "        scaler = amp.GradScaler()\n",
    "#         if 'xlm-roberta' in args.pt:\n",
    "#             net = XLMRobertaForSequenceClassification.from_pretrained(args.pt, num_labels = 7) \n",
    "        \n",
    "#         elif 'klue/roberta' in args.pt:\n",
    "#             net = RobertaForSequenceClassification.from_pretrained(args.pt, num_labels = 7) \n",
    "#         else:\n",
    "#             net = BertForSequenceClassification.from_pretrained(args.pt, num_labels = 7) \n",
    "        net = AutoModelForSequenceClassification.from_pretrained(args.pt, num_labels = 4)\n",
    "        net.to(device)\n",
    "        if len(args.gpu)>1:\n",
    "            net = nn.DataParallel(net)\n",
    "\n",
    "        # ------------------------\n",
    "        # loss\n",
    "        # ------------------------\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # ------------------------\n",
    "        #  Optimizer\n",
    "        # ------------------------\n",
    "        optimizer = optim.Lookahead(optim.RAdam(filter(lambda p: p.requires_grad,net.parameters()), lr=args.start_lr), alpha=0.5, k=5)\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = len(trainloader)*args.epochs)\n",
    "        \n",
    "        \n",
    "        # ----\n",
    "        start_timer = timer()\n",
    "        best_score = 0\n",
    "\n",
    "        for epoch in range(1, args.epochs+1):\n",
    "            train_loss = 0\n",
    "            valid_loss = 0\n",
    "\n",
    "            target_lst = []\n",
    "            pred_lst = []\n",
    "            lr = get_learning_rate(optimizer)\n",
    "            print(f'-------------------')\n",
    "            print(f'{epoch}epoch start')\n",
    "            print(f'-------------------'+'\\n')\n",
    "            print(f'learning rate : {lr : .6f}')\n",
    "            for t, data in enumerate(tqdm.tqdm(trainloader)):\n",
    "\n",
    "                # one iteration update  -------------\n",
    "                ids  = data['ids'].to(device)\n",
    "                mask  = data['mask'].to(device)\n",
    "                tokentype = data['token_type_ids'].to(device)\n",
    "                target = data['targets'].to(device)\n",
    "\n",
    "                # ------------\n",
    "                net.train()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "\n",
    "                if args.amp:\n",
    "                    with amp.autocast():\n",
    "                        # output\n",
    "                        output = net(ids, mask)\n",
    "                        output = output[0]\n",
    "\n",
    "                        # loss\n",
    "                        loss = loss_fn(output, target)\n",
    "                        train_loss += loss\n",
    "\n",
    "\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "\n",
    "                else:\n",
    "                    # output\n",
    "                    output = net(ids, mask)\n",
    "\n",
    "                    # loss\n",
    "                    loss = loss_fn(output, target)\n",
    "                    train_loss += loss\n",
    "\n",
    "                    # update\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "\n",
    "                # for calculate f1 score\n",
    "                target_lst.extend(target.detach().cpu().numpy())\n",
    "                pred_lst.extend(output.argmax(dim=1).tolist())\n",
    "\n",
    "\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step() \n",
    "            train_loss = train_loss / len(trainloader)\n",
    "            train_score = f1_score(y_true=target_lst, y_pred=pred_lst, average='macro')\n",
    "            train_acc = accuracy_score(y_true=target_lst, y_pred=pred_lst)\n",
    "\n",
    "            # validation\n",
    "            valid_loss, valid_score, valid_acc, _ = do_valid(net, validloader)\n",
    "\n",
    "\n",
    "            if valid_acc > best_score:\n",
    "                best_score = valid_acc\n",
    "                best_epoch = epoch\n",
    "                best_loss = valid_loss\n",
    "\n",
    "                torch.save(net.state_dict(), out_dir + f'/{folds}f_{epoch}e_{best_score:.4f}_s.pth')\n",
    "                print('best model saved'+'\\n')\n",
    "\n",
    "\n",
    "            print(f'train loss : {train_loss:.4f}, train f1 score : {train_score : .4f}, train acc : {train_acc : .4f}'+'\\n')\n",
    "            print(f'test loss : {valid_loss:.4f}, test f1 score : {valid_score : .4f}, test acc : {valid_acc : .4f}'+'\\n')\n",
    "\n",
    "\n",
    "        print(f'best test loss : {best_loss : .4f}'+'\\n')\n",
    "        print(f'best epoch : {best_epoch }'+'\\n')\n",
    "        print(f'best accuracy : {best_score : .4f}'+'\\n')\n",
    "        \n",
    "def run_predict(model_path):\n",
    "    ## dataset ------------------------------------\n",
    "    # load\n",
    "    with open(f'./data/{args.pt}/test_data_{args.max_len}.pickle', 'rb') as f:\n",
    "        test_dict = pickle.load(f)\n",
    "        \n",
    "    print('test load')\n",
    "    test_dataset = KobertDataSet(data = test_dict, test=True)\n",
    "    testloader = DataLoader(dataset=test_dataset, batch_size=args.batch_size, \n",
    "                             num_workers=8, shuffle=False, pin_memory=True)\n",
    "    print('set testloader')\n",
    "    ## net ----------------------------------------\n",
    "    scaler = amp.GradScaler()\n",
    "    net = AutoModelForSequenceClassification.from_pretrained(args.pt, num_labels = 4)\n",
    "\n",
    "        \n",
    "    net.to(device)\n",
    "    \n",
    "    if len(args.gpu)>1:\n",
    "        net = nn.DataParallel(net)\n",
    "\n",
    "    f = torch.load(model_path)\n",
    "    net.load_state_dict(f, strict=True)  # True\n",
    "    print('load saved models')\n",
    "    # ------------------------\n",
    "    # validation\n",
    "    preds, logit = do_predict(net, testloader) #outputs\n",
    "           \n",
    "    print('complete predict')\n",
    "    \n",
    "    return preds, np.array(logit)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa4fb075",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "1epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:49<00:00,  6.48it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:33<00:00, 16.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.4512, train f1 score :  0.8389, train acc :  0.8339\n",
      "\n",
      "test loss : 0.3549, test f1 score :  0.8794, test acc :  0.8730\n",
      "\n",
      "-------------------\n",
      "2epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:53<00:00,  6.44it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.3196, train f1 score :  0.8946, train acc :  0.8881\n",
      "\n",
      "test loss : 0.2979, test f1 score :  0.9060, test acc :  0.9000\n",
      "\n",
      "-------------------\n",
      "3epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:51<00:00,  6.46it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:33<00:00, 16.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.2433, train f1 score :  0.9223, train acc :  0.9164\n",
      "\n",
      "test loss : 0.2495, test f1 score :  0.9224, test acc :  0.9163\n",
      "\n",
      "-------------------\n",
      "4epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:46<00:00,  6.52it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.1834, train f1 score :  0.9428, train acc :  0.9379\n",
      "\n",
      "test loss : 0.2025, test f1 score :  0.9389, test acc :  0.9337\n",
      "\n",
      "-------------------\n",
      "5epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:46<00:00,  6.52it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.1396, train f1 score :  0.9570, train acc :  0.9530\n",
      "\n",
      "test loss : 0.2028, test f1 score :  0.9402, test acc :  0.9347\n",
      "\n",
      "-------------------\n",
      "6epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:46<00:00,  6.52it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.1097, train f1 score :  0.9668, train acc :  0.9636\n",
      "\n",
      "test loss : 0.1801, test f1 score :  0.9490, test acc :  0.9462\n",
      "\n",
      "-------------------\n",
      "7epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:46<00:00,  6.51it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0866, train f1 score :  0.9739, train acc :  0.9714\n",
      "\n",
      "test loss : 0.1676, test f1 score :  0.9553, test acc :  0.9520\n",
      "\n",
      "-------------------\n",
      "8epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:46<00:00,  6.52it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0701, train f1 score :  0.9786, train acc :  0.9764\n",
      "\n",
      "test loss : 0.1667, test f1 score :  0.9561, test acc :  0.9532\n",
      "\n",
      "-------------------\n",
      "9epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:46<00:00,  6.52it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0588, train f1 score :  0.9822, train acc :  0.9803\n",
      "\n",
      "test loss : 0.1601, test f1 score :  0.9594, test acc :  0.9567\n",
      "\n",
      "-------------------\n",
      "10epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:46<00:00,  6.52it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0503, train f1 score :  0.9848, train acc :  0.9832\n",
      "\n",
      "test loss : 0.1597, test f1 score :  0.9603, test acc :  0.9578\n",
      "\n",
      "best test loss :  0.1597\n",
      "\n",
      "best epoch : 10\n",
      "\n",
      "best accuracy :  0.9578\n",
      "\n",
      "1 fold pass\n",
      "\n",
      "2 fold pass\n",
      "\n",
      "3 fold pass\n",
      "\n",
      "4 fold pass\n",
      "\n",
      "0 fold pass\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "1epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:48<00:00,  6.49it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.4529, train f1 score :  0.8396, train acc :  0.8327\n",
      "\n",
      "test loss : 0.3603, test f1 score :  0.8791, test acc :  0.8733\n",
      "\n",
      "-------------------\n",
      "2epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:48<00:00,  6.50it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.3215, train f1 score :  0.8942, train acc :  0.8877\n",
      "\n",
      "test loss : 0.2927, test f1 score :  0.9048, test acc :  0.8987\n",
      "\n",
      "-------------------\n",
      "3epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:48<00:00,  6.50it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.2473, train f1 score :  0.9206, train acc :  0.9146\n",
      "\n",
      "test loss : 0.2428, test f1 score :  0.9247, test acc :  0.9189\n",
      "\n",
      "-------------------\n",
      "4epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:49<00:00,  6.48it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:33<00:00, 16.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.1875, train f1 score :  0.9417, train acc :  0.9364\n",
      "\n",
      "test loss : 0.2106, test f1 score :  0.9364, test acc :  0.9314\n",
      "\n",
      "-------------------\n",
      "5epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:54<00:00,  6.42it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.1441, train f1 score :  0.9556, train acc :  0.9514\n",
      "\n",
      "test loss : 0.1958, test f1 score :  0.9418, test acc :  0.9374\n",
      "\n",
      "-------------------\n",
      "6epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:53<00:00,  6.44it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.1128, train f1 score :  0.9652, train acc :  0.9617\n",
      "\n",
      "test loss : 0.1715, test f1 score :  0.9508, test acc :  0.9477\n",
      "\n",
      "-------------------\n",
      "7epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:50<00:00,  6.47it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0898, train f1 score :  0.9723, train acc :  0.9696\n",
      "\n",
      "test loss : 0.1631, test f1 score :  0.9542, test acc :  0.9517\n",
      "\n",
      "-------------------\n",
      "8epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:53<00:00,  6.43it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0714, train f1 score :  0.9780, train acc :  0.9758\n",
      "\n",
      "test loss : 0.1596, test f1 score :  0.9567, test acc :  0.9540\n",
      "\n",
      "-------------------\n",
      "9epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:49<00:00,  6.49it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0592, train f1 score :  0.9816, train acc :  0.9797\n",
      "\n",
      "test loss : 0.1555, test f1 score :  0.9591, test acc :  0.9567\n",
      "\n",
      "-------------------\n",
      "10epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:52<00:00,  6.45it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:33<00:00, 16.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0512, train f1 score :  0.9845, train acc :  0.9828\n",
      "\n",
      "test loss : 0.1539, test f1 score :  0.9606, test acc :  0.9584\n",
      "\n",
      "best test loss :  0.1539\n",
      "\n",
      "best epoch : 10\n",
      "\n",
      "best accuracy :  0.9584\n",
      "\n",
      "2 fold pass\n",
      "\n",
      "3 fold pass\n",
      "\n",
      "4 fold pass\n",
      "\n",
      "0 fold pass\n",
      "\n",
      "1 fold pass\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "1epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:52<00:00,  6.44it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:33<00:00, 16.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.4545, train f1 score :  0.8375, train acc :  0.8318\n",
      "\n",
      "test loss : 0.3526, test f1 score :  0.8834, test acc :  0.8772\n",
      "\n",
      "-------------------\n",
      "2epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:52<00:00,  6.44it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:33<00:00, 16.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.3227, train f1 score :  0.8937, train acc :  0.8871\n",
      "\n",
      "test loss : 0.2912, test f1 score :  0.9050, test acc :  0.8983\n",
      "\n",
      "-------------------\n",
      "3epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:53<00:00,  6.44it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.2473, train f1 score :  0.9202, train acc :  0.9142\n",
      "\n",
      "test loss : 0.2395, test f1 score :  0.9237, test acc :  0.9180\n",
      "\n",
      "-------------------\n",
      "4epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:46<00:00,  6.52it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.1874, train f1 score :  0.9406, train acc :  0.9354\n",
      "\n",
      "test loss : 0.2163, test f1 score :  0.9342, test acc :  0.9294\n",
      "\n",
      "-------------------\n",
      "5epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:46<00:00,  6.53it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.1413, train f1 score :  0.9559, train acc :  0.9518\n",
      "\n",
      "test loss : 0.1794, test f1 score :  0.9475, test acc :  0.9434\n",
      "\n",
      "-------------------\n",
      "6epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:47<00:00,  6.51it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.1101, train f1 score :  0.9661, train acc :  0.9628\n",
      "\n",
      "test loss : 0.1714, test f1 score :  0.9515, test acc :  0.9479\n",
      "\n",
      "-------------------\n",
      "7epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:52<00:00,  6.45it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0871, train f1 score :  0.9734, train acc :  0.9707\n",
      "\n",
      "test loss : 0.1646, test f1 score :  0.9565, test acc :  0.9537\n",
      "\n",
      "-------------------\n",
      "8epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:47<00:00,  6.51it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0703, train f1 score :  0.9785, train acc :  0.9763\n",
      "\n",
      "test loss : 0.1590, test f1 score :  0.9582, test acc :  0.9555\n",
      "\n",
      "-------------------\n",
      "9epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:46<00:00,  6.52it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0580, train f1 score :  0.9823, train acc :  0.9804\n",
      "\n",
      "test loss : 0.1567, test f1 score :  0.9590, test acc :  0.9569\n",
      "\n",
      "-------------------\n",
      "10epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:45<00:00,  6.53it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0502, train f1 score :  0.9846, train acc :  0.9831\n",
      "\n",
      "test loss : 0.1556, test f1 score :  0.9610, test acc :  0.9588\n",
      "\n",
      "best test loss :  0.1556\n",
      "\n",
      "best epoch : 10\n",
      "\n",
      "best accuracy :  0.9588\n",
      "\n",
      "3 fold pass\n",
      "\n",
      "4 fold pass\n",
      "\n",
      "0 fold pass\n",
      "\n",
      "1 fold pass\n",
      "\n",
      "2 fold pass\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "1epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:52<00:00,  6.45it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:33<00:00, 16.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.4591, train f1 score :  0.8352, train acc :  0.8295\n",
      "\n",
      "test loss : 0.3542, test f1 score :  0.8814, test acc :  0.8750\n",
      "\n",
      "-------------------\n",
      "2epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:53<00:00,  6.44it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:33<00:00, 16.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.3216, train f1 score :  0.8937, train acc :  0.8871\n",
      "\n",
      "test loss : 0.2980, test f1 score :  0.9034, test acc :  0.8957\n",
      "\n",
      "-------------------\n",
      "3epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:52<00:00,  6.45it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:33<00:00, 16.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.2479, train f1 score :  0.9208, train acc :  0.9147\n",
      "\n",
      "test loss : 0.2444, test f1 score :  0.9228, test acc :  0.9175\n",
      "\n",
      "-------------------\n",
      "4epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:51<00:00,  6.46it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:33<00:00, 16.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.1897, train f1 score :  0.9412, train acc :  0.9361\n",
      "\n",
      "test loss : 0.2075, test f1 score :  0.9370, test acc :  0.9322\n",
      "\n",
      "-------------------\n",
      "5epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:52<00:00,  6.45it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.1462, train f1 score :  0.9548, train acc :  0.9505\n",
      "\n",
      "test loss : 0.1873, test f1 score :  0.9441, test acc :  0.9400\n",
      "\n",
      "-------------------\n",
      "6epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:45<00:00,  6.53it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.1157, train f1 score :  0.9648, train acc :  0.9613\n",
      "\n",
      "test loss : 0.1846, test f1 score :  0.9456, test acc :  0.9431\n",
      "\n",
      "-------------------\n",
      "7epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:51<00:00,  6.46it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:33<00:00, 16.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0928, train f1 score :  0.9718, train acc :  0.9692\n",
      "\n",
      "test loss : 0.1675, test f1 score :  0.9534, test acc :  0.9506\n",
      "\n",
      "-------------------\n",
      "8epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:51<00:00,  6.45it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0753, train f1 score :  0.9775, train acc :  0.9753\n",
      "\n",
      "test loss : 0.1579, test f1 score :  0.9568, test acc :  0.9539\n",
      "\n",
      "-------------------\n",
      "9epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:52<00:00,  6.44it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0627, train f1 score :  0.9809, train acc :  0.9791\n",
      "\n",
      "test loss : 0.1589, test f1 score :  0.9589, test acc :  0.9562\n",
      "\n",
      "-------------------\n",
      "10epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:44<00:00,  6.54it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0539, train f1 score :  0.9836, train acc :  0.9822\n",
      "\n",
      "test loss : 0.1573, test f1 score :  0.9597, test acc :  0.9574\n",
      "\n",
      "best test loss :  0.1573\n",
      "\n",
      "best epoch : 10\n",
      "\n",
      "best accuracy :  0.9574\n",
      "\n",
      "4 fold pass\n",
      "\n",
      "0 fold pass\n",
      "\n",
      "1 fold pass\n",
      "\n",
      "2 fold pass\n",
      "\n",
      "3 fold pass\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "1epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:45<00:00,  6.53it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.4568, train f1 score :  0.8344, train acc :  0.8296\n",
      "\n",
      "test loss : 0.3532, test f1 score :  0.8824, test acc :  0.8761\n",
      "\n",
      "-------------------\n",
      "2epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:45<00:00,  6.54it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.3203, train f1 score :  0.8943, train acc :  0.8876\n",
      "\n",
      "test loss : 0.2974, test f1 score :  0.9044, test acc :  0.8974\n",
      "\n",
      "-------------------\n",
      "3epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:44<00:00,  6.54it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.2443, train f1 score :  0.9218, train acc :  0.9156\n",
      "\n",
      "test loss : 0.2420, test f1 score :  0.9235, test acc :  0.9179\n",
      "\n",
      "-------------------\n",
      "4epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:44<00:00,  6.54it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.1841, train f1 score :  0.9417, train acc :  0.9368\n",
      "\n",
      "test loss : 0.2083, test f1 score :  0.9389, test acc :  0.9344\n",
      "\n",
      "-------------------\n",
      "5epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:44<00:00,  6.54it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.1393, train f1 score :  0.9570, train acc :  0.9530\n",
      "\n",
      "test loss : 0.1863, test f1 score :  0.9464, test acc :  0.9426\n",
      "\n",
      "-------------------\n",
      "6epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:45<00:00,  6.53it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.1077, train f1 score :  0.9671, train acc :  0.9639\n",
      "\n",
      "test loss : 0.1724, test f1 score :  0.9524, test acc :  0.9496\n",
      "\n",
      "-------------------\n",
      "7epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:45<00:00,  6.54it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0861, train f1 score :  0.9738, train acc :  0.9712\n",
      "\n",
      "test loss : 0.1566, test f1 score :  0.9563, test acc :  0.9536\n",
      "\n",
      "-------------------\n",
      "8epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:45<00:00,  6.54it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0698, train f1 score :  0.9786, train acc :  0.9766\n",
      "\n",
      "test loss : 0.1565, test f1 score :  0.9580, test acc :  0.9559\n",
      "\n",
      "-------------------\n",
      "9epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:44<00:00,  6.54it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0573, train f1 score :  0.9824, train acc :  0.9806\n",
      "\n",
      "test loss : 0.1548, test f1 score :  0.9591, test acc :  0.9575\n",
      "\n",
      "-------------------\n",
      "10epoch start\n",
      "-------------------\n",
      "\n",
      "learning rate :  0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 3433/3433 [08:44<00:00,  6.54it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:32<00:00, 16.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "\n",
      "train loss : 0.0488, train f1 score :  0.9849, train acc :  0.9833\n",
      "\n",
      "test loss : 0.1524, test f1 score :  0.9610, test acc :  0.9592\n",
      "\n",
      "best test loss :  0.1524\n",
      "\n",
      "best epoch : 10\n",
      "\n",
      "best accuracy :  0.9592\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ['monologg/kobert','klue/roberta-base','klue/roberta-small','klue/roberta-large','xlm-roberta-large', \n",
    "#            'bert-base-multilingual-uncased', 'klue/roberta-large']\n",
    "\n",
    "\"\"\"5fold 전용\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    for f in [0,1,2,3,4]:\n",
    "        run_train(folds=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32329f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d141e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396189b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fc713e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63f1e8a2",
   "metadata": {},
   "source": [
    "# ensemble & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc4ed0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efa34f47",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pretrain 다운로드:\n",
    "https://drive.google.com/drive/folders/1cqwv4OQtjCQFfMQkaiO6WYmcHLngg-B5?usp=share_link\n",
    "\"\"\"\n",
    "\n",
    "def ensemble():\n",
    "    final_logit=0\n",
    "    final_logit2=0\n",
    "    \n",
    "#     args.max_len=33\n",
    "#     args.pt = 'klue/roberta-base'\n",
    "#     _, logit1 = run_predict(\"./saved_models/fold3/experiment_name_folder/0f_10e_0.9323_s.pth\")\n",
    "#     _, logit2 = run_predict(\"./saved_models/fold3/experiment_name_folder/1f_10e_0.9321_s.pth\")\n",
    "#     _, logit3 = run_predict(\"./saved_models/fold3/experiment_name_folder/2f_10e_0.9333_s.pth\")\n",
    "#     _, logit4 = run_predict(\"./saved_models/fold3/experiment_name_folder/3f_10e_0.9325_s.pth\")\n",
    "#     _, logit5 = run_predict(\"./saved_models/fold3/experiment_name_folder/4f_10e_0.9321_s.pth\")\n",
    "    \n",
    "#     final_logit += logit1/5\n",
    "#     final_logit += logit2/5\n",
    "#     final_logit += logit3/5\n",
    "#     final_logit += logit4/5\n",
    "#     final_logit += logit5/5\n",
    "    \n",
    "    args.pt = 'klue/roberta-large'\n",
    "    _, logit1 = run_predict(\"./saved_models/fold3/experiment_name_folder/0f_10e_0.9578_s.pth\")\n",
    "    _, logit2 = run_predict(\"./saved_models/fold3/experiment_name_folder/1f_10e_0.9584_s.pth\")\n",
    "    _, logit3 = run_predict(\"./saved_models/fold3/experiment_name_folder/2f_10e_0.9588_s.pth\")\n",
    "    _, logit4 = run_predict(\"./saved_models/fold3/experiment_name_folder/3f_10e_0.9574_s.pth\")\n",
    "    _, logit5 = run_predict(\"./saved_models/fold3/experiment_name_folder/4f_10e_0.9592_s.pth\")\n",
    "    \n",
    "    final_logit2 += logit1/5\n",
    "    final_logit2 += logit2/5\n",
    "    final_logit2 += logit3/5\n",
    "    final_logit2 += logit4/5\n",
    "    final_logit2 += logit5/5\n",
    "    \n",
    "    return final_logit2\n",
    "    \n",
    "    #return final_logit, final_logit2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bb613ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test load\n",
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:08<00:00, 63.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "test load\n",
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:08<00:00, 63.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "test load\n",
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:08<00:00, 62.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "test load\n",
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:08<00:00, 63.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "test load\n",
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:08<00:00, 63.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "test load\n",
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:21<00:00, 24.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "test load\n",
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:21<00:00, 24.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "test load\n",
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:22<00:00, 24.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "test load\n",
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:21<00:00, 24.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n",
      "test load\n",
      "set testloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load saved models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 537/537 [00:21<00:00, 24.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final_logit = ensemble()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80b85117",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['preds'] = final_logit.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3bb90da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final acc : 0.9685\n"
     ]
    }
   ],
   "source": [
    "test_df['preds'] = final_logit.argmax(1)\n",
    "acc = accuracy_score(test_df['preds'], test_df['label'])\n",
    "print(f'final acc : {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01f833d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contents</th>\n",
       "      <th>label</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>놀이기구 부식이 심해 제보합니다. 위치는 의창구 #@주소#이고 조치 좀 해주세요.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>어린이 학원 앞의 도로가 패여 있습니다. 조속한 보수를 해주시기 바랍니다.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>창원시에서 평탄하지 못한 인도로 인하여 보행자 안전사고의 위험이 있습니다. 현장점검...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>차가 다니는 골목길에 주차금지 장애물이 합법인지 문의합니다.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>아파트 뒤편 도로 보수 공사를 요청합니다. 자칫하다가는 큰 사고로 이어질 수 있으니...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34320</th>\n",
       "      <td>진해구 돌아다니고 있는 야생멧돼지를 발견하고 신고 문의 드립니다.</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34321</th>\n",
       "      <td>의창구 #@주소#에 있는 뉴트리아들 포획 부탁드려요.</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34322</th>\n",
       "      <td>문의합니다. 마산합포구 #@주소#의 화장실이 개방형 #@주소#,1, 2층 남녀 화장...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34323</th>\n",
       "      <td>성산구의 생활폐기물매립장 연락처 문의합니다.</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34324</th>\n",
       "      <td>진해시에 환경과에 황#@이름#라는 직원이 있습니까?</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34325 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                contents  label  preds\n",
       "0         놀이기구 부식이 심해 제보합니다. 위치는 의창구 #@주소#이고 조치 좀 해주세요.       0      0\n",
       "1              어린이 학원 앞의 도로가 패여 있습니다. 조속한 보수를 해주시기 바랍니다.      0      0\n",
       "2      창원시에서 평탄하지 못한 인도로 인하여 보행자 안전사고의 위험이 있습니다. 현장점검...      0      0\n",
       "3                      차가 다니는 골목길에 주차금지 장애물이 합법인지 문의합니다.      0      0\n",
       "4      아파트 뒤편 도로 보수 공사를 요청합니다. 자칫하다가는 큰 사고로 이어질 수 있으니...      0      0\n",
       "...                                                  ...    ...    ...\n",
       "34320               진해구 돌아다니고 있는 야생멧돼지를 발견하고 신고 문의 드립니다.      3      3\n",
       "34321                      의창구 #@주소#에 있는 뉴트리아들 포획 부탁드려요.      3      3\n",
       "34322  문의합니다. 마산합포구 #@주소#의 화장실이 개방형 #@주소#,1, 2층 남녀 화장...      3      3\n",
       "34323                          성산구의 생활폐기물매립장 연락처 문의합니다.       3      3\n",
       "34324                       진해시에 환경과에 황#@이름#라는 직원이 있습니까?      3      3\n",
       "\n",
       "[34325 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafd568e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0de24f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
